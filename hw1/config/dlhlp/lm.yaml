data:
  corpus:                                 # Pass to dataloader
    # The following depends on corpus
    name: 'Dlhlp'                         # Specify corpus
    path: 'data_dir'
    train_split: ['train']                # Official LM src
    dev_split: ['dev']
    bucketing: True
    batch_size: 32
  text:
    mode: 'character'                     # 'character'/'word'/'subword'
    vocab_file: 'vocab.txt'

hparas:                                   # Experiment hyper-parameters
  valid_step: 250
  max_step: 20000
  optimizer: 'Adam'
  lr: 0.0001
  eps: 0.00000001
  lr_scheduler: 'fixed'                    # 'fixed'/'warmup'
  
model:                                     # Model architecture
  emb_tying: False                         # https://arxiv.org/pdf/1608.05859.pdf
  emb_dim: 1024
  module: 'LSTM'                           # 'LSTM'/'GRU'
  dim: 1024
  n_layers: 2
  dropout: 0.5
